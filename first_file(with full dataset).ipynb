{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKY9MvIa7Gjs"
      },
      "source": [
        "#Importing libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRFEcH3i7SCv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfuhvF_y-8hl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#loading the dataset to pandas dataframe\n",
        "credit_card_data=pd.read_csv('/content/drive/MyDrive/ML LAB/creditcard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "xEZ2KH5N_Mg3",
        "outputId": "1a4db6df-453c-4e0b-afe8-ac5b085306bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#first five rows of the dataset\n",
        "credit_card_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "Nb7oH_UrAOZa",
        "outputId": "6cdb52ea-9a01-4ef2-a5ad-876776b08f6c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>172786.0</td>\n",
              "      <td>-11.881118</td>\n",
              "      <td>10.071785</td>\n",
              "      <td>-9.834783</td>\n",
              "      <td>-2.066656</td>\n",
              "      <td>-5.364473</td>\n",
              "      <td>-2.606837</td>\n",
              "      <td>-4.918215</td>\n",
              "      <td>7.305334</td>\n",
              "      <td>1.914428</td>\n",
              "      <td>...</td>\n",
              "      <td>0.213454</td>\n",
              "      <td>0.111864</td>\n",
              "      <td>1.014480</td>\n",
              "      <td>-0.509348</td>\n",
              "      <td>1.436807</td>\n",
              "      <td>0.250034</td>\n",
              "      <td>0.943651</td>\n",
              "      <td>0.823731</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>172787.0</td>\n",
              "      <td>-0.732789</td>\n",
              "      <td>-0.055080</td>\n",
              "      <td>2.035030</td>\n",
              "      <td>-0.738589</td>\n",
              "      <td>0.868229</td>\n",
              "      <td>1.058415</td>\n",
              "      <td>0.024330</td>\n",
              "      <td>0.294869</td>\n",
              "      <td>0.584800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.214205</td>\n",
              "      <td>0.924384</td>\n",
              "      <td>0.012463</td>\n",
              "      <td>-1.016226</td>\n",
              "      <td>-0.606624</td>\n",
              "      <td>-0.395255</td>\n",
              "      <td>0.068472</td>\n",
              "      <td>-0.053527</td>\n",
              "      <td>24.79</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>1.919565</td>\n",
              "      <td>-0.301254</td>\n",
              "      <td>-3.249640</td>\n",
              "      <td>-0.557828</td>\n",
              "      <td>2.630515</td>\n",
              "      <td>3.031260</td>\n",
              "      <td>-0.296827</td>\n",
              "      <td>0.708417</td>\n",
              "      <td>0.432454</td>\n",
              "      <td>...</td>\n",
              "      <td>0.232045</td>\n",
              "      <td>0.578229</td>\n",
              "      <td>-0.037501</td>\n",
              "      <td>0.640134</td>\n",
              "      <td>0.265745</td>\n",
              "      <td>-0.087371</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.026561</td>\n",
              "      <td>67.88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>-0.240440</td>\n",
              "      <td>0.530483</td>\n",
              "      <td>0.702510</td>\n",
              "      <td>0.689799</td>\n",
              "      <td>-0.377961</td>\n",
              "      <td>0.623708</td>\n",
              "      <td>-0.686180</td>\n",
              "      <td>0.679145</td>\n",
              "      <td>0.392087</td>\n",
              "      <td>...</td>\n",
              "      <td>0.265245</td>\n",
              "      <td>0.800049</td>\n",
              "      <td>-0.163298</td>\n",
              "      <td>0.123205</td>\n",
              "      <td>-0.569159</td>\n",
              "      <td>0.546668</td>\n",
              "      <td>0.108821</td>\n",
              "      <td>0.104533</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>172792.0</td>\n",
              "      <td>-0.533413</td>\n",
              "      <td>-0.189733</td>\n",
              "      <td>0.703337</td>\n",
              "      <td>-0.506271</td>\n",
              "      <td>-0.012546</td>\n",
              "      <td>-0.649617</td>\n",
              "      <td>1.577006</td>\n",
              "      <td>-0.414650</td>\n",
              "      <td>0.486180</td>\n",
              "      <td>...</td>\n",
              "      <td>0.261057</td>\n",
              "      <td>0.643078</td>\n",
              "      <td>0.376777</td>\n",
              "      <td>0.008797</td>\n",
              "      <td>-0.473649</td>\n",
              "      <td>-0.818267</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>0.013649</td>\n",
              "      <td>217.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Time         V1         V2        V3        V4        V5  \\\n",
              "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
              "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
              "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
              "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
              "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
              "\n",
              "              V6        V7        V8        V9  ...       V21       V22  \\\n",
              "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
              "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
              "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
              "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
              "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
              "\n",
              "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
              "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
              "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
              "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
              "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
              "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
              "\n",
              "        Class  \n",
              "284802      0  \n",
              "284803      0  \n",
              "284804      0  \n",
              "284805      0  \n",
              "284806      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "credit_card_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sobVu2wfAe8h",
        "outputId": "89a59603-8d8d-4b93-d8b9-b73ce74c34c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n"
          ]
        }
      ],
      "source": [
        "#dataset information\n",
        "credit_card_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQW8hY6OBI4C",
        "outputId": "bc2899df-4d1d-407e-bfd4-5b8e7de2d65d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Time      0\n",
              "V1        0\n",
              "V2        0\n",
              "V3        0\n",
              "V4        0\n",
              "V5        0\n",
              "V6        0\n",
              "V7        0\n",
              "V8        0\n",
              "V9        0\n",
              "V10       0\n",
              "V11       0\n",
              "V12       0\n",
              "V13       0\n",
              "V14       0\n",
              "V15       0\n",
              "V16       0\n",
              "V17       0\n",
              "V18       0\n",
              "V19       0\n",
              "V20       0\n",
              "V21       0\n",
              "V22       0\n",
              "V23       0\n",
              "V24       0\n",
              "V25       0\n",
              "V26       0\n",
              "V27       0\n",
              "V28       0\n",
              "Amount    0\n",
              "Class     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checking the number of missing values in each column\n",
        "credit_card_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "sSv37nf67O1m",
        "outputId": "9625a146-8693-4cc9-9dee-0db377005a6f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Class</th>\n",
              "      <th>normAmount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>0</td>\n",
              "      <td>0.244964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.342475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>0</td>\n",
              "      <td>1.160686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>0</td>\n",
              "      <td>0.140534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.073403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>-11.881118</td>\n",
              "      <td>10.071785</td>\n",
              "      <td>-9.834783</td>\n",
              "      <td>-2.066656</td>\n",
              "      <td>-5.364473</td>\n",
              "      <td>-2.606837</td>\n",
              "      <td>-4.918215</td>\n",
              "      <td>7.305334</td>\n",
              "      <td>1.914428</td>\n",
              "      <td>4.356170</td>\n",
              "      <td>...</td>\n",
              "      <td>0.213454</td>\n",
              "      <td>0.111864</td>\n",
              "      <td>1.014480</td>\n",
              "      <td>-0.509348</td>\n",
              "      <td>1.436807</td>\n",
              "      <td>0.250034</td>\n",
              "      <td>0.943651</td>\n",
              "      <td>0.823731</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.350151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>-0.732789</td>\n",
              "      <td>-0.055080</td>\n",
              "      <td>2.035030</td>\n",
              "      <td>-0.738589</td>\n",
              "      <td>0.868229</td>\n",
              "      <td>1.058415</td>\n",
              "      <td>0.024330</td>\n",
              "      <td>0.294869</td>\n",
              "      <td>0.584800</td>\n",
              "      <td>-0.975926</td>\n",
              "      <td>...</td>\n",
              "      <td>0.214205</td>\n",
              "      <td>0.924384</td>\n",
              "      <td>0.012463</td>\n",
              "      <td>-1.016226</td>\n",
              "      <td>-0.606624</td>\n",
              "      <td>-0.395255</td>\n",
              "      <td>0.068472</td>\n",
              "      <td>-0.053527</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.254117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>1.919565</td>\n",
              "      <td>-0.301254</td>\n",
              "      <td>-3.249640</td>\n",
              "      <td>-0.557828</td>\n",
              "      <td>2.630515</td>\n",
              "      <td>3.031260</td>\n",
              "      <td>-0.296827</td>\n",
              "      <td>0.708417</td>\n",
              "      <td>0.432454</td>\n",
              "      <td>-0.484782</td>\n",
              "      <td>...</td>\n",
              "      <td>0.232045</td>\n",
              "      <td>0.578229</td>\n",
              "      <td>-0.037501</td>\n",
              "      <td>0.640134</td>\n",
              "      <td>0.265745</td>\n",
              "      <td>-0.087371</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.026561</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.081839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>-0.240440</td>\n",
              "      <td>0.530483</td>\n",
              "      <td>0.702510</td>\n",
              "      <td>0.689799</td>\n",
              "      <td>-0.377961</td>\n",
              "      <td>0.623708</td>\n",
              "      <td>-0.686180</td>\n",
              "      <td>0.679145</td>\n",
              "      <td>0.392087</td>\n",
              "      <td>-0.399126</td>\n",
              "      <td>...</td>\n",
              "      <td>0.265245</td>\n",
              "      <td>0.800049</td>\n",
              "      <td>-0.163298</td>\n",
              "      <td>0.123205</td>\n",
              "      <td>-0.569159</td>\n",
              "      <td>0.546668</td>\n",
              "      <td>0.108821</td>\n",
              "      <td>0.104533</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.313249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>-0.533413</td>\n",
              "      <td>-0.189733</td>\n",
              "      <td>0.703337</td>\n",
              "      <td>-0.506271</td>\n",
              "      <td>-0.012546</td>\n",
              "      <td>-0.649617</td>\n",
              "      <td>1.577006</td>\n",
              "      <td>-0.414650</td>\n",
              "      <td>0.486180</td>\n",
              "      <td>-0.915427</td>\n",
              "      <td>...</td>\n",
              "      <td>0.261057</td>\n",
              "      <td>0.643078</td>\n",
              "      <td>0.376777</td>\n",
              "      <td>0.008797</td>\n",
              "      <td>-0.473649</td>\n",
              "      <td>-0.818267</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>0.013649</td>\n",
              "      <td>0</td>\n",
              "      <td>0.514355</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>284315 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               V1         V2        V3        V4        V5        V6  \\\n",
              "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
              "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
              "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
              "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
              "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
              "...           ...        ...       ...       ...       ...       ...   \n",
              "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
              "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
              "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
              "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
              "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
              "\n",
              "              V7        V8        V9       V10  ...       V21       V22  \\\n",
              "0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
              "1      -0.078803  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672   \n",
              "2       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
              "3       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
              "4       0.592941 -0.270533  0.817739  0.753074  ... -0.009431  0.798278   \n",
              "...          ...       ...       ...       ...  ...       ...       ...   \n",
              "284802 -4.918215  7.305334  1.914428  4.356170  ...  0.213454  0.111864   \n",
              "284803  0.024330  0.294869  0.584800 -0.975926  ...  0.214205  0.924384   \n",
              "284804 -0.296827  0.708417  0.432454 -0.484782  ...  0.232045  0.578229   \n",
              "284805 -0.686180  0.679145  0.392087 -0.399126  ...  0.265245  0.800049   \n",
              "284806  1.577006 -0.414650  0.486180 -0.915427  ...  0.261057  0.643078   \n",
              "\n",
              "             V23       V24       V25       V26       V27       V28  Class  \\\n",
              "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0   \n",
              "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   \n",
              "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0   \n",
              "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0   \n",
              "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0   \n",
              "...          ...       ...       ...       ...       ...       ...    ...   \n",
              "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731      0   \n",
              "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527      0   \n",
              "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561      0   \n",
              "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533      0   \n",
              "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649      0   \n",
              "\n",
              "        normAmount  \n",
              "0         0.244964  \n",
              "1        -0.342475  \n",
              "2         1.160686  \n",
              "3         0.140534  \n",
              "4        -0.073403  \n",
              "...            ...  \n",
              "284802   -0.350151  \n",
              "284803   -0.254117  \n",
              "284804   -0.081839  \n",
              "284805   -0.313249  \n",
              "284806    0.514355  \n",
              "\n",
              "[284315 rows x 30 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "credit_card_data['normAmount'] = StandardScaler().fit_transform(credit_card_data['Amount'].values.reshape(-1, 1))\n",
        "data = credit_card_data.drop(['Time','Amount'],axis=1)\n",
        "#class 1 data present in data set\n",
        "data[data.Class==1]\n",
        "data[data.Class==0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jsId4yp4W2R"
      },
      "source": [
        "#Checking the Target Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRpdhpBfBjEs",
        "outputId": "1b6eb7c5-3303-4837-8c8c-49615b59fd61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    284315\n",
              "1       492\n",
              "Name: Class, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#distribution of legit transaction and fraud transaction\n",
        "credit_card_data['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "rfSJYH2F4bah",
        "outputId": "2d615118-8fe7-4777-d8af-f378a7a551b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='Class', ylabel='count'>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.countplot(x='Class', data=credit_card_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdZu_6gFGR4V"
      },
      "source": [
        "This dataset is highly unbalanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEGQZklWGXnt"
      },
      "source": [
        "0 --> Normal Transaction\n",
        "\n",
        "1 --> Fraudulent transaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGlBv3sS42KT"
      },
      "source": [
        "This is a clear example where using a typical accuracy score to evaluate our classification algorithm. For example, if we just used a majority class to assign values to all records, we will still be having a high accuracy, BUT WE WOULD BE CLASSIFYING ALL \"1\" INCORRECTLY!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XssXGmqL54En"
      },
      "source": [
        "#Resampling the dataset\n",
        "\n",
        "Essentially this is a method that will process the data to have an approximate 50-50 ratio.\n",
        "\n",
        "One way to achieve this is by OVER-sampling, which is adding copies of the under-represented class (better when you have little data)\n",
        "\n",
        "Another is UNDER-sampling, which deletes instances from the over-represented class (better when he have lot's of data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FaGhfuA9eix"
      },
      "outputs": [],
      "source": [
        "X = data.loc[:, data.columns != 'Class']\n",
        "y = data.loc[:, data.columns == 'Class']\n",
        "#print(y.sum())\n",
        "# print(X)\n",
        "#shows y dataset is of column class which has both 0 and 1\n",
        "# y[y.Class==1]\n",
        "# y[y.Class==0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5hw6iiB80G7"
      },
      "source": [
        "#Applying SMOTE with Over Sampling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWfRfUg285JV"
      },
      "source": [
        "As we mentioned earlier, there are several ways to resample skewed data. Apart from under and over sampling, there is a very popular approach called SMOTE (Synthetic Minority Over-Sampling Technique), which is a combination of oversampling and undersampling, but the oversampling approach is not by replicating minority class but constructing new minority class data instance via an algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gka6glaf8VAi",
        "outputId": "4234ee9a-de9b-4d37-bc12-95620fca481d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of fraud counts in original dataset:0.1727485630620034%\n",
            "Percentage of fraud counts in the new data:50.0%\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import SMOTE \n",
        "#Over sampling\n",
        "sm = SMOTE(random_state=42, sampling_strategy='auto')\n",
        "\n",
        "X_sampled,y_sampled = sm.fit_resample(X,y)\n",
        "\n",
        "#Percentage of fraudlent records in original data\n",
        "Source_data_no_fraud_count = len(data[data.Class==0])\n",
        "Source_data_fraud_count = len(data[data.Class==1])\n",
        "print('Percentage of fraud counts in original dataset:{}%'.format((Source_data_fraud_count*100)/(Source_data_no_fraud_count+Source_data_fraud_count)))\n",
        "\n",
        "#Percentage of fraudlent records in sampled data\n",
        "Sampled_data_no_fraud_count = len(y_sampled[y_sampled==0])\n",
        "Sampled_data_fraud_count = len(y_sampled[y_sampled==1])\n",
        "print('Percentage of fraud counts in the new data:{}%'.format((Sampled_data_fraud_count*100)/(Sampled_data_no_fraud_count+Sampled_data_fraud_count)))\n",
        "# X_sampled.head(5)\n",
        "# print(y_sampled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trvsqUChNlmt"
      },
      "source": [
        "#Split the data into trainning data and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGl1VDjtMSe3",
        "outputId": "03399370-4b69-4041-aa0e-8ad986990d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number transactions train dataset:  199364\n",
            "Number transactions test dataset:  85443\n",
            "Total number of transactions:  284807\n"
          ]
        }
      ],
      "source": [
        "# Whole dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 5)\n",
        "\n",
        "print(\"Number transactions train dataset: \", len(X_train))\n",
        "print(\"Number transactions test dataset: \", len(X_test))\n",
        "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
        "\n",
        "# #both ytrain n ytest have 0 & 1 class\n",
        "# print(y_train.shape)\n",
        "# print(y_train[y_train.Class==1])\n",
        "# print(y_train[y_train.Class==0])\n",
        "# print(y_test.shape)\n",
        "# print(y_test[y_test.Class==1])\n",
        "# print(y_test[y_test.Class==0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4GhA_50_jLA",
        "outputId": "8e521a34-1ecd-4b13-fd8d-38ef816e3c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number transactions train dataset:  398041\n",
            "Number transactions test dataset:  170589\n",
            "Total number of transactions:  568630\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# oversampled dataset\n",
        "X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = train_test_split(X_sampled ,y_sampled ,test_size = 0.3  ,random_state = 5)\n",
        "print(\"\")\n",
        "print(\"Number transactions train dataset: \", len(X_train_sampled))\n",
        "print(\"Number transactions test dataset: \", len(X_test_sampled))\n",
        "print(\"Total number of transactions: \", len(X_train_sampled)+len(X_test_sampled))\n",
        "\n",
        "\n",
        "# X_train_sampled_df = pd.DataFrame(X_train_sampled)\n",
        "# y_train_sampled_df = pd.DataFrame(y_train_sampled)\n",
        "# X_test_sampled_df = pd.DataFrame(X_test_sampled)\n",
        "# y_test_sampled_df = pd.DataFrame(y_test_sampled)\n",
        "# X_train_sampled_df.head(10)\n",
        "# y_train_sampled_df.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG04ooo7D2NC"
      },
      "source": [
        "We are very interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions. If you think how Accuracy, Precision and Recall work for a confusion matrix, recall would be the most interesting:\n",
        "\n",
        "Accuracy = (TP+TN)/total\n",
        "Precision = TP/(TP+FP)\n",
        "Recall = TP/(TP+FN)\n",
        "\n",
        "As we know, due to the imbalacing of the data, many observations could be predicted as False Negatives, being, that we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this.\n",
        "\n",
        "Obviously, trying to increase recall, tends to come with a decrease of precision. However, in our case, if we predict that a transaction is fraudulent and turns out not to be, is not a massive problem compared to the opposite.\n",
        "\n",
        "We could even apply a cost function when having FN and FP with different weights for each type of error, but let's leave that aside for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aQt_hKOeGuZ"
      },
      "source": [
        "#Model Training\n",
        "\n",
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z001pHzwD87R"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report,accuracy_score,precision_score,f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omQZuIMxGgSS"
      },
      "source": [
        "#HYPERTUNING FUNCTION\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATrCVZYke6ey"
      },
      "outputs": [],
      "source": [
        "def logistic_regression(x_train,x_test,y_train,y_test):\n",
        "  c_param_range = [0.01,0.1,1,10,100]\n",
        "  solver=[\"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
        "  penaltys={ \"lbfgs\":  ['l2'] , \"liblinear\":['l1', 'l2'] , \"sag\":['l2'], \"saga\": [ \"l1\", \"l2\"] }\n",
        "  li=list()\n",
        "  cval=None \n",
        "  sol=None \n",
        "  pen=None\n",
        "  rec_max=0\n",
        "  for solve in solver: \n",
        "    for penalty in penaltys[solve]:\n",
        "      for c in c_param_range:\n",
        "        lr = LogisticRegression(C = c, penalty = penalty ,solver=solve  )\n",
        "        lr.fit(x_train,y_train)\n",
        "        y_pred=lr.predict(x_test)\n",
        "        recall=recall_score(y_test,y_pred)\n",
        "        accuracy=accuracy_score(y_test,y_pred)\n",
        "        precision=precision_score(y_test,y_pred)\n",
        "        f1score=f1_score(y_test,y_pred)\n",
        "\n",
        "        classification =(classification_report(y_test, y_pred))\n",
        "        li.append([solve,penalty,c,recall,accuracy,precision,f1score])\n",
        "        if recall>rec_max:\n",
        "          cval=c\n",
        "          sol=solve\n",
        "          pen=penalty\n",
        "          rec_max=recall\n",
        "\n",
        "  \n",
        "  print(\"Maximum Values Obtained at : \")\n",
        "  print(\"Solver =\",sol)\n",
        "  print(\"Penalty =\",pen)\n",
        "  print(\"C =\",cval)\n",
        "  print(\"Recall =\",rec_max)\n",
        "  return li \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-rhkz_49XU3"
      },
      "source": [
        "#Hypertuning on the original dataset\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC4HZXHoqHYs"
      },
      "outputs": [],
      "source": [
        "y_train=np.array(y_train).ravel()\n",
        "y_test=np.array(y_test).ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOJw4_QEpN5S",
        "outputId": "801a1ccf-9478-4baa-ed04-6f7796bd0879"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum Values Obtained at : \n",
            "Solver = lbfgs\n",
            "Penalty = l2\n",
            "C = 1\n",
            "Recall = 0.6081081081081081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        }
      ],
      "source": [
        "li=logistic_regression(X_train, X_test, y_train, y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1vx8eWgxDAd",
        "outputId": "d2a8900e-3f4c-48f6-8639-d46f4a9ea53f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Solver Penalty       C  Recall Score  Accuracy  Precision  F1-Score\n",
            "0       lbfgs      l2    0.01      0.574324  0.999146   0.894737  0.699588\n",
            "1       lbfgs      l2    0.10      0.601351  0.999181   0.890000  0.717742\n",
            "2       lbfgs      l2    1.00      0.608108  0.999192   0.891089  0.722892\n",
            "3       lbfgs      l2   10.00      0.601351  0.999181   0.890000  0.717742\n",
            "4       lbfgs      l2  100.00      0.601351  0.999181   0.890000  0.717742\n",
            "5   liblinear      l1    0.01      0.547297  0.999111   0.900000  0.680672\n",
            "6   liblinear      l1    0.10      0.587838  0.999169   0.896907  0.710204\n",
            "7   liblinear      l1    1.00      0.608108  0.999192   0.891089  0.722892\n",
            "8   liblinear      l1   10.00      0.601351  0.999181   0.890000  0.717742\n",
            "9   liblinear      l1  100.00      0.601351  0.999181   0.890000  0.717742\n",
            "10  liblinear      l2    0.01      0.554054  0.999122   0.901099  0.686192\n",
            "11  liblinear      l2    0.10      0.567568  0.999134   0.893617  0.694215\n",
            "12  liblinear      l2    1.00      0.608108  0.999192   0.891089  0.722892\n",
            "13  liblinear      l2   10.00      0.601351  0.999181   0.890000  0.717742\n",
            "14  liblinear      l2  100.00      0.601351  0.999181   0.890000  0.717742\n",
            "15        sag      l2    0.01      0.560811  0.999134   0.902174  0.691667\n",
            "16        sag      l2    0.10      0.567568  0.999146   0.903226  0.697095\n",
            "17        sag      l2    1.00      0.574324  0.999157   0.904255  0.702479\n",
            "18        sag      l2   10.00      0.574324  0.999157   0.904255  0.702479\n",
            "19        sag      l2  100.00      0.574324  0.999157   0.904255  0.702479\n",
            "20       saga      l1    0.01      0.533784  0.999087   0.897727  0.669492\n",
            "21       saga      l1    0.10      0.567568  0.999146   0.903226  0.697095\n",
            "22       saga      l1    1.00      0.560811  0.999134   0.902174  0.691667\n",
            "23       saga      l1   10.00      0.560811  0.999134   0.902174  0.691667\n",
            "24       saga      l1  100.00      0.560811  0.999134   0.902174  0.691667\n",
            "25       saga      l2    0.01      0.554054  0.999122   0.901099  0.686192\n",
            "26       saga      l2    0.10      0.560811  0.999134   0.902174  0.691667\n",
            "27       saga      l2    1.00      0.560811  0.999134   0.902174  0.691667\n",
            "28       saga      l2   10.00      0.560811  0.999134   0.902174  0.691667\n",
            "29       saga      l2  100.00      0.560811  0.999134   0.902174  0.691667\n"
          ]
        }
      ],
      "source": [
        "df=pd.DataFrame(li,columns=[\"Solver\",\"Penalty\",\"C\",\"Recall Score\",\"Accuracy\",\"Precision\",\"F1-Score\"])\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xfHs39f9bu3"
      },
      "source": [
        "#Hypertuning on the resampled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nagqM55_6Dl"
      },
      "outputs": [],
      "source": [
        "y_train_sampled=np.array(y_train_sampled).ravel()\n",
        "y_test_sampled=np.array(y_test_sampled).ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV8Bfp_n9fMY",
        "outputId": "9625494e-b346-418f-db1e-8d6c0829a5fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "C:\\Users\\Akshay\\anaconda3\\envs\\gputest\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum Values Obtained at : \n",
            "Solver = liblinear\n",
            "Penalty = l2\n",
            "C = 10\n",
            "Recall = 0.917191487369185\n"
          ]
        }
      ],
      "source": [
        "li2=logistic_regression(X_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y85mHE-s_F3_",
        "outputId": "5dbd9df9-22d3-4072-b641-68013ae94df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Solver Penalty       C  Recall Score  Accuracy  Precision  F1-Score\n",
            "0       lbfgs      l2    0.01      0.914347  0.945477   0.975229  0.943807\n",
            "1       lbfgs      l2    0.10      0.916899  0.946591   0.974956  0.945037\n",
            "2       lbfgs      l2    1.00      0.917133  0.946708   0.974962  0.945164\n",
            "3       lbfgs      l2   10.00      0.917156  0.946726   0.974975  0.945182\n",
            "4       lbfgs      l2  100.00      0.917156  0.946726   0.974975  0.945182\n",
            "5   liblinear      l1    0.01      0.913188  0.944926   0.975259  0.943203\n",
            "6   liblinear      l1    0.10      0.916524  0.946403   0.974946  0.944833\n",
            "7   liblinear      l1    1.00      0.916793  0.946526   0.974929  0.944968\n",
            "8   liblinear      l1   10.00      0.917016  0.946614   0.974886  0.945066\n",
            "9   liblinear      l1  100.00      0.916840  0.946550   0.974930  0.944993\n",
            "10  liblinear      l2    0.01      0.914160  0.945348   0.975151  0.943671\n",
            "11  liblinear      l2    0.10      0.916911  0.946591   0.974944  0.945037\n",
            "12  liblinear      l2    1.00      0.917180  0.946714   0.974927  0.945172\n",
            "13  liblinear      l2   10.00      0.917191  0.946732   0.974952  0.945190\n",
            "14  liblinear      l2  100.00      0.917191  0.946732   0.974952  0.945190\n",
            "15        sag      l2    0.01      0.914324  0.945465   0.975228  0.943794\n",
            "16        sag      l2    0.10      0.916840  0.946562   0.974955  0.945005\n",
            "17        sag      l2    1.00      0.917016  0.946644   0.974947  0.945095\n",
            "18        sag      l2   10.00      0.917028  0.946638   0.974923  0.945090\n",
            "19        sag      l2  100.00      0.917028  0.946638   0.974923  0.945090\n",
            "20       saga      l1    0.01      0.913457  0.945002   0.975144  0.943293\n",
            "21       saga      l1    0.10      0.916583  0.946450   0.974984  0.944882\n",
            "22       saga      l1    1.00      0.916840  0.946538   0.974906  0.944982\n",
            "23       saga      l1   10.00      0.916875  0.946556   0.974907  0.945001\n",
            "24       saga      l1  100.00      0.916887  0.946562   0.974907  0.945007\n",
            "25       saga      l2    0.01      0.914253  0.945436   0.975238  0.943762\n",
            "26       saga      l2    0.10      0.916758  0.946544   0.975001  0.944983\n",
            "27       saga      l2    1.00      0.916840  0.946550   0.974930  0.944993\n",
            "28       saga      l2   10.00      0.916887  0.946562   0.974907  0.945007\n",
            "29       saga      l2  100.00      0.916887  0.946562   0.974907  0.945007\n"
          ]
        }
      ],
      "source": [
        "df=pd.DataFrame(li2,columns=[\"Solver\",\"Penalty\",\"C\",\"Recall Score\",\"Accuracy\",\"Precision\",\"F1-Score\"])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQwn-zkl9PkB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsSiZMsdceAd"
      },
      "source": [
        "#RAMDOM FOREST\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCtxDBXFW6BV"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# random forest model creation\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(X_train_sampled, y_train_sampled)\n",
        "# predictions\n",
        "yPred = rfc.predict(X_test_sampled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ClstkAkwPpM",
        "outputId": "f1b9f453-db22-41cc-9d4c-8300e5fea595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[85149    14]\n",
            " [    0 85426]]\n",
            "0.9999179314023765\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85163\n",
            "           1       1.00      1.00      1.00     85426\n",
            "\n",
            "    accuracy                           1.00    170589\n",
            "   macro avg       1.00      1.00      1.00    170589\n",
            "weighted avg       1.00      1.00      1.00    170589\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prediction = rfc.predict(X_test_sampled)\n",
        "print(confusion_matrix(y_test_sampled, prediction))\n",
        "print(accuracy_score(y_test_sampled, prediction))\n",
        "print(classification_report(y_test_sampled, prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byBJ7AhxwZSD"
      },
      "source": [
        "#SUPPORT VECTOR MACHINES (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol6s0cItwg57"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        " # \"Support vector classifier creation\"  \n",
        "classifier = SVC(kernel='sigmoid', random_state=0)  \n",
        "classifier.fit(X_train_sampled, y_train_sampled)\n",
        "# predictions\n",
        "yPred_SVC = classifier.predict(X_test_sampled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTV7GB17xoDM",
        "outputId": "a7d6e3b6-91c9-4c51-8c2d-08d94e3d28b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[73398 11765]\n",
            " [11461 73965]]\n",
            "0.8638481965425672\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86     85163\n",
            "           1       0.86      0.87      0.86     85426\n",
            "\n",
            "    accuracy                           0.86    170589\n",
            "   macro avg       0.86      0.86      0.86    170589\n",
            "weighted avg       0.86      0.86      0.86    170589\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prediction = classifier.predict(X_test_sampled)\n",
        "print(confusion_matrix(y_test_sampled, prediction))\n",
        "print(accuracy_score(y_test_sampled, prediction))\n",
        "print(classification_report(y_test_sampled, prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJWB-AfU1RDY"
      },
      "source": [
        "#Using GridsearchCV on SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJhwA52z1UUo",
        "outputId": "01464ed2-df5a-463d-ca08-88c5f41b6192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
            "[CV 1/5] END .......C=0.1, gamma=1, kernel=rbf;, score=0.992 total time=261.4min\n",
            "[CV 2/5] END .......C=0.1, gamma=1, kernel=rbf;, score=0.991 total time=257.6min\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# defining parameter range\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']} \n",
        "  \n",
        "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
        "  \n",
        "# fitting the model for grid search\n",
        "grid.fit(X_train_sampled, y_train_sampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa764WWRJVjz"
      },
      "outputs": [],
      "source": [
        "prediction = .predict(X_test_sampled)\n",
        "print(confusion_matrix(y_test_sampled, prediction))\n",
        "print(accuracy_score(y_test_sampled, prediction))\n",
        "print(classification_report(y_test_sampled, prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsgfPfWsuqep"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}